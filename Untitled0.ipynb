{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIwUpkCoTa0u1aC+oMTawX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashajyothi02/Honey/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkfEd2L0XB2s",
        "outputId": "2fad81c3-cb7c-4c03-b1de-dbad5d3c2fcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'NLP', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# Download punkt library\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Now you should be able to tokenize your text\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"NLTK is a powerful library for NLP.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download the Wordnet resource\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Create stemmer and lemmatizer instances\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Word to be processed\n",
        "word = \"running\"\n",
        "\n",
        "# Perform stemming and lemmatization\n",
        "stemmed_word = stemmer.stem(word)\n",
        "lemmatized_word = lemmatizer.lemmatize(word)\n",
        "\n",
        "# Display the results\n",
        "print(\"Stemmed Word:\", stemmed_word)\n",
        "print(\"Lemmatized Word:\", lemmatized_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3ua94S9bPRy",
        "outputId": "4beecf6c-f094-46bf-ab41-7ac49d524b84"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Word: run\n",
            "Lemmatized Word: running\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#Download the averaged_perception_tagger resource\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(\"NLTK is a powerful tool for NLP\")\n",
        "\n",
        "# Perform part-of-speech tagging\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "# Display the results\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr6RSi8QcrCt",
        "outputId": "bb4cec25-9169-4f36-f804-5a00a0673288"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('powerful', 'JJ'), ('tool', 'NN'), ('for', 'IN'), ('NLP', 'NNP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import ne_chunk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the words resource\n",
        "nltk.download('words')\n",
        "\n",
        "# Download other necessary resources\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Tokenize the text\n",
        "text = \"Bill Gates is the founder of Microsoft\"\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Perform part-of-speech tagging\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "# Perform named entity reconition\n",
        "ner_tags = ne_chunk(pos_tags)\n",
        "\n",
        "# Display the results\n",
        "print(ner_tags)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec9pcnVKeUMV",
        "outputId": "8cb2d877-06ad-44fb-b3da-1d2d3968f432"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Bill/NNP)\n",
            "  (GPE Gates/NNP)\n",
            "  is/VBZ\n",
            "  the/DT\n",
            "  founder/NN\n",
            "  of/IN\n",
            "  (ORGANIZATION Microsoft/NNP))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample documents\n",
        "documents = [\"Bag-of-Words is a common NLP technique.\",\n",
        "             \"It simplifies text data for various tasks.\",\n",
        "             \"Bag-of-Words is used in text classification.\"]\n",
        "\n",
        "# Create a CountVectorizer instance\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit an d transform the documents into a bag-of-words representation\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get the feature names (vocabulary)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Display the results\n",
        "print(\"Bag-of-Words Representation:\")\n",
        "print(X.toarray())\n",
        "print(\"\\nVocabulary:\")\n",
        "print(feature_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_AncTLXjazh",
        "outputId": "dcb75497-b81d-4b93-9f46-121f91e5ff36"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag-of-Words Representation:\n",
            "[[1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1]\n",
            " [0 0 0 1 1 0 0 1 0 0 1 1 0 1 0 1 0]\n",
            " [1 1 0 0 0 1 1 0 0 1 0 0 0 1 1 0 1]]\n",
            "\n",
            "Vocabulary:\n",
            "['bag' 'classification' 'common' 'data' 'for' 'in' 'is' 'it' 'nlp' 'of'\n",
            " 'simplifies' 'tasks' 'technique' 'text' 'used' 'various' 'words']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing libraries for Chatbot**"
      ],
      "metadata": {
        "id": "yo7MR4sEncki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.chat.util import Chat, reflections"
      ],
      "metadata": {
        "id": "Gxae7iRRnRlX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Patterns & Responses**"
      ],
      "metadata": {
        "id": "1b1oXQctnlBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patterns = [(r'hi|hello|hey', ['Hello!, ', 'Hi there!, ', 'Hey!'])]"
      ],
      "metadata": {
        "id": "5MhcqbVYnmA1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cOXW940an_D8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot = Chat(patterns, reflections)"
      ],
      "metadata": {
        "id": "WqhXwZH-n_9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def waste_managemet_chat():\n",
        "  print(\"Welcome to the Waste Management Chatbot\")\n",
        "  while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == 'quit' :\n",
        "      print(\"Goodbye!\")\n",
        "      break\n",
        "    else:\n",
        "      response = chatbot.response(user_input)\n",
        "      print(\"Chatbot:\", response)"
      ],
      "metadata": {
        "id": "IPcOLZQxocVZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}